{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as s\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class gaussian_naive_bayes:\n",
    "    \n",
    "    \"\"\"Instantiate a Gaussian Naive Bayes Object with the following parameters:\n",
    "    \n",
    "    features            : A dataframe consisting of continuous features, exluding labels\n",
    "    labels              : A series consisting of binary labels\n",
    "    data_split_ratio    : A tuple consisting of data splitting ratio\n",
    "    apply_pca           : Boolean value specifying whether to apply PCA or not\n",
    "    n_components        : Number of Eigen Vectors having Non Zero values to keep\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(xerox_copy,features,labels,data_split_ratio,apply_pca,n_components):\n",
    "        \n",
    "        xerox_copy.binary_labels = np.array(labels).reshape(labels.shape[0],1)     \n",
    "        \n",
    "        xerox_copy.split_ratio = data_split_ratio\n",
    "        \n",
    "        xerox_copy.n_principal_components = n_components\n",
    "        \n",
    "        xerox_copy.unique_labels = list(labels.unique()) #We are doing Monkey Patching ['B','M']\n",
    "        \n",
    "        if apply_pca == True:\n",
    "            \n",
    "            xerox_copy.X_new = xerox_copy.apply_dim_reduction(features,xerox_copy.n_principal_components) #We are doing Monkey Patching\n",
    "            \n",
    "            \n",
    "            \n",
    "    def apply_dim_reduction(xerox_copy,data,n_components):\n",
    "        \n",
    "        X = np.array(data)\n",
    "        \n",
    "        mu = np.mean(X,axis=0)\n",
    "        \n",
    "        mu = mu.reshape(-1,mu.shape[0])\n",
    "        \n",
    "        X_dash = X - mu\n",
    "        \n",
    "        sigma_hat = (1/data.shape[0])*np.matmul(X_dash.T,X_dash)\n",
    "        \n",
    "        sigma_hat_decompose = np.linalg.svd(sigma_hat)\n",
    "        \n",
    "        Q = sigma_hat_decompose[0]\n",
    "        \n",
    "        xerox_copy.Q_tilda = Q[:,0:n_components]\n",
    "        \n",
    "        X_new = np.matmul(X_dash,xerox_copy.Q_tilda)\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "    \n",
    "    def data_splitting(xerox_copy,data):\n",
    "        \n",
    "        new_data = data.iloc[:,0:data.shape[1]-1]\n",
    "        \n",
    "        new_data['label'] = data['class']\n",
    "        \n",
    "        training_data_len = int(xerox_copy.split_ratio[0]*new_data.shape[0])\n",
    "        \n",
    "        neg_training_data = new_data[new_data['label'] == xerox_copy.unique_labels[0]].iloc[0:training_data_len//2]\n",
    "\n",
    "        pos_training_data = new_data[new_data['label'] == xerox_copy.unique_labels[1]].iloc[0:training_data_len//2]\n",
    "        \n",
    "        training_data = pd.concat([neg_training_data,pos_training_data])\n",
    "        \n",
    "        cv_data_len = int(xerox_copy.split_ratio[1]*new_data.shape[0])\n",
    "        \n",
    "        neg_remaining_data = new_data[new_data['label'] == xerox_copy.unique_labels[0]].iloc[training_data_len//2:]\n",
    "\n",
    "        pos_remaining_data = new_data[new_data['label'] == xerox_copy.unique_labels[1]].iloc[training_data_len//2:]\n",
    "        \n",
    "        remaining_data = pd.concat([neg_remaining_data,pos_remaining_data])\n",
    "        \n",
    "        cv_data = remaining_data.iloc[0:cv_data_len]\n",
    "\n",
    "        testing_data = remaining_data.iloc[cv_data_len:]\n",
    "        \n",
    "        return training_data,cv_data,testing_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_rru_gaussian_nb(xerox_copy,data):\n",
    "        \n",
    "        mu_hat_pos = np.array(data[data['label'] == xerox_copy.unique_labels[1]].iloc[:,0:xerox_copy.n_principal_components].mean())\n",
    "\n",
    "        sigma_hat_pos = np.array(data[data['label'] == xerox_copy.unique_labels[1]].iloc[:,0:xerox_copy.n_principal_components].cov())\n",
    "        \n",
    "        mu_hat_neg = np.array(data[data['label'] == xerox_copy.unique_labels[0]].iloc[:,0:xerox_copy.n_principal_components].mean())\n",
    "\n",
    "        sigma_hat_neg = np.array(data[data['label'] == xerox_copy.unique_labels[0]].iloc[:,0:xerox_copy.n_principal_components].cov())\n",
    "        \n",
    "        xerox_copy.neg_likelihood_params = (mu_hat_neg,sigma_hat_neg) #We are using Monkey Patching again here\n",
    "        \n",
    "        xerox_copy.pos_likelihood_params = (mu_hat_pos,sigma_hat_pos) #We are using Monkey Patching again here\n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate(xerox_copy,data):\n",
    "    \n",
    "        inputs = np.array(data.iloc[:,0:xerox_copy.n_principal_components])\n",
    "    \n",
    "        posterior_pos = s.multivariate_normal.pdf(inputs,xerox_copy.pos_likelihood_params[0],xerox_copy.pos_likelihood_params[1]) \n",
    "    \n",
    "        posterior_neg = s.multivariate_normal.pdf(inputs,xerox_copy.neg_likelihood_params[0],xerox_copy.neg_likelihood_params[1])\n",
    "    \n",
    "        boolean_mask = posterior_pos > posterior_neg\n",
    "    \n",
    "        predicted_category = pd.Series(boolean_mask)\n",
    "    \n",
    "        predicted_category.replace(to_replace=[False,True],value=[xerox_copy.unique_labels[0],xerox_copy.unique_labels[1]],inplace=True)\n",
    "    \n",
    "        predicted_results = np.array(predicted_category)\n",
    "        \n",
    "        actual_results = np.array(data['label'])\n",
    "        \n",
    "        print(classification_report(actual_results,predicted_results,target_names=xerox_copy.unique_labels))\n",
    "\n",
    "\n",
    "# So, now we need to evlaluate the following Probability: \n",
    "# \n",
    "# \\begin{equation}\n",
    "# P(Diagnosis = M | radius mean = x) = P(radius mean = x | Diagnosis = M) P(texturemean = y| Diagnosis = M) P(Diagnosis = M)\n",
    "# \\end{equation}\n",
    "# \n",
    "# Now, in order to evaluate the likelihood probability P(radiusmean = x | Diagnosis = M) P(texturemean = y | Diagnosis = M) which is given by Multivariate Joint Gaussian Distribution PDF. Now, for this PDF, we need to find out the best estimate of the parameters of Multivariate Joint Normal Distribution because we are assuming that our Malignant tumor training data is being sampled from a Multivariate Joint Normal (Gaussian) Distribution. The two parameters will be namely: mu_hat_m and sigma_hat_m.\n",
    "# \n",
    "# \\begin{equation}\n",
    "# P(radiusmean = x | Diagnosis = M)P(texturemean = y | Diagnosis = M) = \\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{rM}}}e^{-\\frac{(x-\\hat{\\mu_\\text{rM}})^2}{2\\hat{\\sigma_\\text{rM}^2}}}\\right)\\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{tM}}}e^{-\\frac{(y-\\hat{\\mu_\\text{tM}})^2}{2\\hat{\\sigma_\\text{tM}^2}}}\\right)\n",
    "# \\end{equation}\n",
    "\n",
    "# So, now we need to evlaluate the following Probability: \n",
    "# \n",
    "# \\begin{equation}\n",
    "# P(Diagnosis = B | radius mean = x) = P(radius mean = x | Diagnosis = B) P(texturemean = y| Diagnosis = B) P(Diagnosis = B)\n",
    "# \\end{equation}\n",
    "# \n",
    "# Now, in order to evaluate the likelihood probability P(radiusmean = x | Diagnosis = B) P(texturemean = y | Diagnosis = B) which is given by Multivariate Joint Gaussian Distribution PDF. Now, for this PDF, we need to find out the best estimate of the parameters of Multivariate Joint Normal Distribution because we are assuming that our Malignant tumor training data is being sampled from a Multivariate Joint Normal (Gaussian) Distribution. The two parameters will be namely: mu_hat_b and sigma_hat_b.\n",
    "# \n",
    "# \\begin{equation}\n",
    "# P(radiusmean = x | Diagnosis = B)P(texturemean = y | Diagnosis = B) = \\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{rB}}}e^{-\\frac{(x-\\hat{\\mu_\\text{rB}})^2}{2\\hat{\\sigma_\\text{rB}^2}}}\\right)\\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{tB}}}e^{-\\frac{(y-\\hat{\\mu_\\text{tB}})^2}{2\\hat{\\sigma_\\text{tB}^2}}}\\right)\n",
    "# \\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
